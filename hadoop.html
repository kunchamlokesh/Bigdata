<title>HADOOP INSTALLATION </title>
<div dir="ltr" style="text-align: left;" trbidi="on">
<span style="background-color: #c27ba0;"><span></span></span><br />
<div class="separator" style="clear: both; text-align: center;">
<a href="hadoop.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="66" src="hadoop.png" width="640" /></a></div>
<br />
<div class="subtitle_2nd" id="java">
INITIALLY YOU HAVE TO INSTALL JAVA HERE I HAVE GIVEN</div>
<br />
<pre><span style="background-color: #c27ba0;">lokesh@lenova-G500:~$ cd ~

# Update the source list
lokesh@lenova-G500:~$ sudo apt-get update

# The OpenJDK project is the default version of Java 
# that is provided from a supported Ubuntu repository.
lokesh@lenova-G500:~$ sudo apt-get install default-jdk

lokesh@lenova-G500:~$ java -version
java version "1.7.0_65"
OpenJDK Runtime Environment (IcedTea 2.5.3) (7u71-2.5.3-0ubuntu0.14.04.1)
OpenJDK 64-Bit Server VM (build 24.65-b04, mixed mode)&nbsp;</span></pre>
<pre>&nbsp;</pre>
<div class="subtitle_2nd" id="user">
CREATE A HADOOP GROUP<br />
</div>
<div class="subtitle_2nd" id="user">
</div>
<pre><span style="background-color: #c27ba0;">lokesh@lenova-G500:~$ sudo addgroup hadoop
Adding group `hadoop' (GID 1002) ...
Done.

lokesh@lenova-G500:~$ sudo adduser --ingroup hadoop lokesh
Adding user `hduser' ...
Adding new user `hduser' (1001) with group `hadoop' ...
Creating home directory `/home/lokesh' ...
Copying files from `/etc/skel' ...
Enter new UNIX password: 
Retype new UNIX password: 
passwd: password updated successfully
Changing the user information for hduser
Enter the new value, or press ENTER for the default
	Full Name []: 
	Room Number []: 
	Work Phone []: 
	Home Phone []: 
	Other []: 
Is the information correct? [Y/n] Y</span></pre>
<pre>&nbsp;</pre>
<pre>&nbsp;</pre>
<div class="subtitle_2nd" id="ssh">
Installing SSH</div>
<b>ssh</b> has two main components:<br />
<ol>
<li><b>ssh</b> : The command we use to connect to remote machines - the client. </li>
<li><b>sshd</b> : The daemon that is running on the server and allows clients to connect to the server.</li>
</ol>
The <b>ssh</b> is pre-enabled on Linux, but in order to start <b>sshd</b> daemon, we need to install <b>ssh</b> first. Use this command to do that :<br />
<br />
<pre><span style="background-color: #c27ba0;">lokesh@lenova-G500:~$ sudo apt-get install ssh</span></pre>
<pre>&nbsp;</pre>
This will install ssh on our machine. If we get something similar to the following, we can think it is setup properly:<br />
<br />
<pre><span style="background-color: #c27ba0;">lokesh@lenova-G500:~$ which ssh
/usr/bin/ssh

lokesh@lenova-G500:~$ which sshd
/usr/sbin/sshd</span></pre>
<pre><span style="background-color: #c27ba0;">&nbsp;</span></pre>
<pre>&nbsp;</pre>
<pre>&nbsp;</pre>
<div class="subtitle_2nd" id="ssh">
Create and Setup SSH Certificates</div>
<div class="subtitle_2nd" id="ssh">
</div>
Hadoop requires SSH access to manage its nodes, i.e. remote machines 
plus our local machine. For our single-node setup of Hadoop, we 
therefore need to configure SSH access to localhost.<br />
So, we need to have SSH up and running on our machine and configured it to allow SSH public key authentication.<br />
Hadoop uses SSH (to access its nodes) which would normally require 
the user to enter a password. However, this requirement can be 
eliminated by creating and setting up SSH certificates using the 
following commands. If asked for a filename just leave it blank and 
press the enter key to continue.<br />
<br />
<pre><span style="background-color: #c27ba0;">lokesh@lenova-G500:~$ su hduser
Password: 
lokesh@lenova-G500:~$ ssh-keygen -t rsa -P ""
Generating public/private rsa key pair.
Enter file in which to save the key (/home/lokesh/.ssh/id_rsa): 
Created directory '/home/hduser/.ssh'.
Your identification has been saved in /home/lokesh/.ssh/id_rsa.
Your public key has been saved in /home/lokesh/.ssh/id_rsa.pub.
The key fingerprint is:
50:6b:f3:fc:0f:32:bf:30:79:c2:41:71:26:cc:7d:e3 lokesh@lenova-G500
The key's randomart image is:
+--[ RSA 2048]----+
|        .oo.o    |
|       . .o=. o  |
|      . + .  o . |
|       o =    E  |
|        S +      |
|         . +     |
|          O +    |
|           O o   |
|            o..  |
+-----------------+


lokesh@lenova-G500:/home/lokesh$ cat $HOME/.ssh/id_rsa.pub &gt;&gt; $HOME/.ssh/authorized_keys&nbsp;</span></pre>
<pre>&nbsp;</pre>
The second command adds the newly created key to the list of 
authorized keys<br />
<br />
so that Hadoop can use ssh without prompting for a 
password.<br />
We can check if ssh works:<br />
<br />
<pre><span style="background-color: #c27ba0;">lokesh@lenova-G500:/home/k$ ssh localhost
The authenticity of host 'localhost (127.0.0.1)' can't be established.
ECDSA key fingerprint is e1:8b:a0:a5:75:ef:f4:b4:5e:a9:ed:be:64:be:5c:2f.
Are you sure you want to continue connecting (yes/no)? yes</span>
</pre>
<pre>&nbsp;</pre>
<pre>&nbsp;</pre>
<h3 style="text-align: left;">
<b><span style="font-weight: normal;">Install Hadoop</span></b></h3>
<pre>&nbsp; </pre>
<pre><span style="background-color: #c27ba0;">lokesh@lenova-G500:~$ wget http://mirrors.sonic.net/apache/hadoop/common/hadoop-2.6.0/hadoop-2.6.0.tar.gz
lokesh@lenova-G500:~$ tar xvzf hadoop-2.6.0.tar.gz </span></pre>
<br />
We want to move the Hadoop installation to the <b>/usr/local/hadoop</b> directory using the following command:<br />
<br />
<pre><span style="background-color: #c27ba0;">lokesh@lenova-G500:~/hadoop-2.6.0$ su k
Password: 

k@laptop:/home/hduser$ sudo adduser hduser sudo
[sudo] password for k: 
Adding user `hduser' to group `sudo' ...
Adding user hduser to group sudo
Done.</span></pre>
<pre><span style="background-color: #c27ba0;">&nbsp;</span></pre>
<pre><span style="background-color: #c27ba0;">lokesh@lenova-G500:/home/hduser$ sudo su hduser

lokesh@lenova-G500:~/hadoop-2.6.0$ sudo mv * /usr/local/hadoop 
lokesh@lenova-G500:~/hadoop-2.6.0$ sudo chown -R hduser:hadoop /usr/local/hadoop </span></pre>
<br />
<div class="subtitle_2nd" id="SetupConfigurationFiles">
Setup Configuration Files</div>
The following files will have to be modified to complete the Hadoop setup:<br />
<ol>
<li>~/.bashrc</li>
<li>/usr/local/hadoop/etc/hadoop/hadoop-env.sh</li>
<li>/usr/local/hadoop/etc/hadoop/core-site.xml</li>
<li>/usr/local/hadoop/etc/hadoop/mapred-site.xml.template</li>
<li>/usr/local/hadoop/etc/hadoop/hdfs-site.xml</li>
</ol>
<b>1. ~/.bashrc</b>:<br />
<br />
Before editing the <b>.bashrc</b> file in our home directory, we need to find the path where Java has been installed to set the <b>JAVA_HOME</b> environment variable using the following command:<br />
<br />
<pre></pre>
<pre><span style="background-color: #c27ba0;">hduser@laptop update-alternatives --config java
There is only one alternative in link group java (providing /usr/bin/java): <span style="color: blue;">/usr/lib/jvm/java-7-openjdk-amd64/jre/bin/java</span>
Nothing to configure.</span></pre>
<pre>&nbsp;</pre>
<pre>&nbsp;</pre>
<div style="text-align: left;">
Now we can append the following to the end of <b>~/.bashrc</b>:</div>
<div style="text-align: left;">
<br /></div>
<div style="text-align: left;">
<span style="background-color: #c27ba0;"><br /></span></div>
<pre><span style="background-color: #c27ba0;">lokesh@lenova-G500:~$ vi ~/.bashrc

#HADOOP VARIABLES START
export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64
export HADOOP_INSTALL=/usr/local/hadoop
export PATH=$PATH:$HADOOP_INSTALL/bin
export PATH=$PATH:$HADOOP_INSTALL/sbin
export HADOOP_MAPRED_HOME=$HADOOP_INSTALL
export HADOOP_COMMON_HOME=$HADOOP_INSTALL
export HADOOP_HDFS_HOME=$HADOOP_INSTALL
export YARN_HOME=$HADOOP_INSTALL
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_INSTALL/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_INSTALL/lib"
#HADOOP VARIABLES END

lokesh@lenova-G500:~$ source ~/.bashrc</span></pre>
<pre>&nbsp;</pre>
<div style="text-align: left;">
note that the JAVA_HOME should be set as the path just before the '.../bin/':</div>
<div style="text-align: left;">
<br /></div>
<pre><span style="background-color: #c27ba0;">lokesh@lenova-G500-VirtualBox:~$ javac -version
javac 1.7.0_75

lokesh@lenova-G500-VirtualBox:~$ which javac
/usr/bin/javac

lokesh@lenova-G500-VirtualBox:~$ readlink -f /usr/bin/javac
/usr/lib/jvm/java-7-openjdk-amd64/bin/javac
</span></pre>
<div style="text-align: left;">
<br /></div>
<b>2. /usr/local/hadoop/etc/hadoop/hadoop-env.sh</b><br />
We need to set <b>JAVA_HOME</b> by modifying <b>hadoop-env.sh</b> file.<br />
<br />
<pre><span style="background-color: #c27ba0;">lokesh@lenova-G500:~$ vi /usr/local/hadoop/etc/hadoop/hadoop-env.sh

export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64</span></pre>
<pre>&nbsp;</pre>
<div style="text-align: left;">
Adding the above statement in the <b>hadoop-env.sh</b> file ensures that the value of JAVA_HOME variable will be available to Hadoop whenever it is started up.</div>
<div style="text-align: left;">
<br /></div>
<div style="text-align: left;">
&nbsp;<b>3. /usr/local/hadoop/etc/hadoop/core-site.xml</b>:</div>
<div style="text-align: left;">
<br /></div>
<div style="text-align: left;">
The <b>/usr/local/hadoop/etc/hadoop/core-site.xml</b> file contains configuration properties that Hadoop uses when starting up. <br />
This file can be used to override the default settings that Hadoop starts with.&nbsp;&nbsp;</div>
<div style="text-align: left;">
<span style="background-color: #c27ba0;"><br /></span></div>
<pre><span style="background-color: #c27ba0;">lokesh@lenova-G500:~$ sudo mkdir -p /app/hadoop/tmp
lokesh@lenova-G500:~$ sudo chown hduser:hadoop /app/hadoop/tmp</span></pre>
<pre><span style="background-color: #c27ba0;">&nbsp;</span></pre>
<pre><span style="background-color: #c27ba0;">hduser@laptop:~$ vi /usr/local/hadoop/etc/hadoop/core-site.xml

&lt;configuration&gt;
 &lt;property&gt;
  &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
  &lt;value&gt;/app/hadoop/tmp&lt;/value&gt;
  &lt;description&gt;A base for other temporary directories.&lt;/description&gt;
 &lt;/property&gt;

 &lt;property&gt;
  &lt;name&gt;fs.default.name&lt;/name&gt;
  &lt;value&gt;hdfs://localhost:54310&lt;/value&gt;
  &lt;description&gt;The name of the default file system.  A URI whose
  scheme and authority determine the FileSystem implementation.  The
  uri's scheme determines the config property (fs.SCHEME.impl) naming
  the FileSystem implementation class.  The uri's authority is used to
  determine the host, port, etc. for a filesystem.&lt;/description&gt;
 &lt;/property&gt;
&lt;/configuration&gt;</span></pre>
<pre>&nbsp;</pre>
<b>4. /usr/local/hadoop/etc/hadoop/mapred-site.xml</b><br />
By default, the <b>/usr/local/hadoop/etc/hadoop/</b> folder contains <br />
<b>/usr/local/hadoop/etc/hadoop/mapred-site.xml.template</b> <br />
file which has to be renamed/copied with the name <b>mapred-site.xml</b><br />
<br />
<pre><span style="background-color: #c27ba0;">lokesh@lenova-G500:~$ cp /usr/local/hadoop/etc/hadoop/mapred-site.xml.template /usr/local/hadoop/etc/hadoop/mapred-site.xml</span></pre>
<br />
<b>The <b>mapred-site.xml</b> file is used to specify which framework is being used for MapReduce.<br />
 We need to 
enter the following content in between the &lt;configuration&gt;&lt;/configuration&gt; tag:&nbsp;&nbsp;</b><br />
<span style="background-color: #c27ba0;"><b><br /></b></span>
<br />
<pre><span style="background-color: #c27ba0;">&lt;configuration&gt;
 &lt;property&gt;
  &lt;name&gt;mapred.job.tracker&lt;/name&gt;
  &lt;value&gt;localhost:54311&lt;/value&gt;
  &lt;description&gt;The host and port that the MapReduce job tracker runs
  at.  If "local", then jobs are run in-process as a single map
  and reduce task.
  &lt;/description&gt;
 &lt;/property&gt;
&lt;/configuration&gt;</span></pre>
<pre>&nbsp;</pre>
<b>5. /usr/local/hadoop/etc/hadoop/hdfs-site.xml</b><br />
The <b>/usr/local/hadoop/etc/hadoop/hdfs-site.xml</b> file needs to be configured for each host in the cluster that is being used. <br />
It is used to specify the directories which will be used as the <b>namenode</b> and the <b>datanode</b> on that host. <br />
Before editing this file, we need to create two directories which 
will contain the namenode and the datanode for this Hadoop installation.
 <br />
This can be done using the following commands:<br />
<br />
<pre><span style="background-color: #c27ba0;">lokesh@lenova-G500:~$ sudo mkdir -p /usr/local/hadoop_store/hdfs/namenode
lokesh@lenova-G500:~$ sudo mkdir -p /usr/local/hadoop_store/hdfs/datanode
lokesh@lenova-G500:~$ sudo chown -R hduser:hadoop /usr/local/hadoop_store&nbsp;</span></pre>
<pre>&nbsp;</pre>
<div style="text-align: left;">
Open the file and enter the following content in between the &lt;configuration&gt;&lt;/configuration&gt; tag:&nbsp;&nbsp;</div>
<div style="text-align: left;">
<br /></div>
<pre><span style="background-color: #c27ba0;">lokesh@lenova-G500:~$ vi /usr/local/hadoop/etc/hadoop/hdfs-site.xml

&lt;configuration&gt;
 &lt;property&gt;
  &lt;name&gt;dfs.replication&lt;/name&gt;
  &lt;value&gt;1&lt;/value&gt;
  &lt;description&gt;Default block replication.
  The actual number of replications can be specified when the file is created.
  The default is used if replication is not specified in create time.
  &lt;/description&gt;
 &lt;/property&gt;
 &lt;property&gt;
   &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
   &lt;value&gt;file:/usr/local/hadoop_store/hdfs/namenode&lt;/value&gt;
 &lt;/property&gt;
 &lt;property&gt;
   &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
   &lt;value&gt;file:/usr/local/hadoop_store/hdfs/datanode&lt;/value&gt;
 &lt;/property&gt;
&lt;/configuration&gt;</span></pre>
<pre>&nbsp;</pre>
<div class="subtitle_2nd" id="Format_Hadoop_Filesystem">
Format the New Hadoop Filesystem</div>
Now, the Hadoop file system needs to be formatted so that we can 
start to use it. The format command should be issued with write 
permission since it creates <b>current</b> directory <br />
under <b>/usr/local/hadoop_store/hdfs/namenode</b> folder:<br />
<br />
<pre><span style="background-color: #c27ba0;">lokesh@lenova-G500:~$ hadoop namenode -format
DEPRECATED: Use of this script to execute hdfs command is deprecated.
Instead use the hdfs command for it.</span></pre>
<pre>&nbsp;</pre>
<div class="subtitle_2nd" id="Start_Hadoop">
Starting Hadoop</div>
Now it's time to start the newly installed single node cluster. <br />
We can use <b>start-all.sh</b> or (<b>start-dfs.sh</b> and <b>start-yarn.sh</b>)<br />
<span style="background-color: #c27ba0;"><br /></span>
<pre><span style="background-color: #c27ba0;">lokesh@lenova-G500:~$ start-all.sh
This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh
16/01/21 3:21:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Starting namenodes on [localhost]
localhost: starting namenode, logging to /usr/local/hadoop/logs/hadoop-lokesh-namenode-lenova-G500.out
localhost: starting datanode, logging to /usr/local/hadoop/logs/hadoop-hduser-datanode-laptop.out
Starting secondary namenodes [0.0.0.0]
0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-lokesh-secondarynamenode-lenova-G500.out
16/01/21 3:21:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
starting yarn daemons
starting resourcemanager, logging to /usr/local/hadoop/logs/yarn-lokesh-resourcemanager-lenova-G500.out
localhost: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-hduser-nodemanager-lenova-G500.out</span></pre>
<br />
<br />
<br />
<pre> </pre>
<br />
<pre>&nbsp;</pre>
<br />
<pre>&nbsp;</pre>
<pre>&nbsp;</pre>
<br />
<div style="text-align: left;">
<br /></div>
<div style="text-align: left;">
<br /></div>
<br />
<pre>&nbsp;</pre>
<br />
<pre>&nbsp;</pre>
</div>

